{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   ipywidgets import interactive, fixed\n",
    "import matplotlib.pyplot as plt\n",
    "from   model.cnn import CNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from   scipy.ndimage import gaussian_filter1d\n",
    "from   scipy.stats import norm\n",
    "from   source.dataloader import BatchgenWrapper, TrainingDataGenerator, ValidationDataGenerator\n",
    "from   source.metrics import roc_auc\n",
    "from   source.preprocessing import get_preprocessed_polyp_segmentation_mask_info, train_validation_split\n",
    "from   source.visualization import plotter_batch, plotter_gradcam\n",
    "from   tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from   tensorflow.keras.optimizers import Adam, SGD\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"preprocessed-data\"\n",
    "\n",
    "# Number of trained estimators in the ensemble\n",
    "n_estimators = 3\n",
    "\n",
    "# Image properties\n",
    "raw_image_size = (100,100,100)\n",
    "patch_size     = (50,50,50) # Field-of-view of the network\n",
    "n_channels     = 1 # 1 = CT image only, 2 = CT image + manual expert segmentation mask\n",
    "\n",
    "# Training\n",
    "batch_size = 1\n",
    "train_size = 0.8 # Train-Validation Split\n",
    "\n",
    "# Model\n",
    "persistance        = True\n",
    "pretrained         = True\n",
    "pretrained_weights = 'weights/noseg_cnn_pretraining'\n",
    "\n",
    "# Fix global seed (as good as possible) to ensure reproducibility of results\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load information of the CT scans from 'ct_info.csv' and get a list of the preprocessed ct scans and segmentation masks that are available in 'preprocessed-data/'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ct_info = pd.read_csv('ct_info.csv')\n",
    "df_ct_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_info = get_preprocessed_polyp_segmentation_mask_info(data_dir)\n",
    "df_preprocessed_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge information into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_ct_info.merge(df_preprocessed_info, how='inner', on=['patient', 'polyp', 'segmentation'])\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test training datagenerator and augmentations (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = BatchgenWrapper(data=df_data,\n",
    "                                       batch_size=batch_size,\n",
    "                                       raw_image_size=raw_image_size,\n",
    "                                       patch_size=patch_size,\n",
    "                                       n_channels=n_channels,\n",
    "                                       one_hot=False,\n",
    "                                       num_processes=1,\n",
    "                                       num_cached_per_queue=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch   = train_data_generator[0]\n",
    "X_test_batch = test_batch[0]\n",
    "y_test_batch = test_batch[1]\n",
    "print('X_test_batch:', X_test_batch.shape, ', y_test_batch:', y_test_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(plotter_batch,\n",
    "            batch        = fixed(test_batch),\n",
    "            sample_nr    = (0,X_test_batch.shape[0]-1),\n",
    "            channel      = (0,X_test_batch.shape[4]-1),\n",
    "            slice_x      = (0,X_test_batch.shape[1]-1),\n",
    "            slice_y      = (0,X_test_batch.shape[2]-1),\n",
    "            slice_z      = (0,X_test_batch.shape[3]-1),\n",
    "            cmap         = [\"gist_yarg\", \"cool\", \"inferno\", \"magma\", \"plasma\", \"viridis\"],\n",
    "            reverse_cmap = [True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(input_shape=(50, 50, 50, n_channels), classes=1, dropout=0.1, mc=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrained:\n",
    "    # Get layer names\n",
    "    layer_names = [layer.name for layer in model.layers]\n",
    "    print('network layers:', layer_names)\n",
    "\n",
    "    # Freeze layers\n",
    "    for layer_name in layer_names[:9]:\n",
    "        model.get_layer(layer_name).trainable = True\n",
    "\n",
    "    # Verify trainability\n",
    "    for layer in model.layers:\n",
    "        print('layer:', layer.name, 'trainable:', layer.trainable)\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.01),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy', roc_auc])\n",
    "\n",
    "if pretrained:\n",
    "    model.load_weights(pretrained_weights)\n",
    "\n",
    "initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_es = EarlyStopping(monitor='val_loss',\n",
    "                      mode='min',\n",
    "                      patience=4*((df_data.shape[0]*train_size)//batch_size),\n",
    "                      restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for est in range(n_estimators):\n",
    "    \n",
    "    print('\\nEstimator #{:d}\\n'.format(est))\n",
    "    \n",
    "    # Perform dataset split (unique seed for each estimator)\n",
    "    df_data_train, df_data_valid = train_validation_split(df_data, train_size=train_size, random_state=est)\n",
    "    print(\"\\nDatasets: Train set =\", df_data_train.shape[0], \", Validation set =\", df_data_valid.shape[0])\n",
    "    \n",
    "\n",
    "    # Training Datagenerator\n",
    "    train_data_generator = BatchgenWrapper(data=df_data_train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           raw_image_size=raw_image_size,\n",
    "                                           patch_size=patch_size,\n",
    "                                           n_channels=n_channels,\n",
    "                                           one_hot=False,\n",
    "                                           num_processes=1,\n",
    "                                           num_cached_per_queue=1)\n",
    "    \n",
    "    # Validation Datagenerator\n",
    "    valid_data_generator = ValidationDataGenerator(data=df_data_valid,\n",
    "                                                   batch_size=df_data_valid.shape[0],\n",
    "                                                   patch_size=patch_size,\n",
    "                                                   n_channels=n_channels,\n",
    "                                                   num_threads=1,\n",
    "                                                   shuffle=False)\n",
    "    \n",
    "    # Check batches\n",
    "    train_test_batch = train_data_generator[0]\n",
    "    print('\\nTrain batch:', train_test_batch[0].shape)\n",
    "    \n",
    "    valid_test_batch = valid_data_generator[0]\n",
    "    print('Valid batch:', valid_test_batch[0].shape)\n",
    "        \n",
    "    # Set initial weights\n",
    "    model.set_weights(initial_weights)\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(train_data_generator,\n",
    "                        epochs           = 1000,\n",
    "                        validation_data  = valid_data_generator,\n",
    "                        verbose          = 0,\n",
    "                        callbacks        = [cb_es])\n",
    "    history = history.history\n",
    "    \n",
    "    # Plot model history\n",
    "    keys    = ['loss','roc_auc']\n",
    "    fig, ax = plt.subplots(1, len(keys), figsize=(8*len(keys),6), num=0, clear=True)\n",
    "    for i, key in enumerate(keys):\n",
    "        ax[i].plot(history[key], c='orange', alpha=0.5)\n",
    "        ax[i].plot(history['val_'+key], c='lightblue', alpha=0.5)\n",
    "        ax[i].plot(gaussian_filter1d(history[key], 3), c='red', lw=2, label='training')\n",
    "        ax[i].plot(gaussian_filter1d(history['val_'+key], 3), c='blue', lw=2, label='validation')\n",
    "        ax[i].legend(fontsize=16)\n",
    "        ax[i].set_xlabel('epoch', fontsize=20)\n",
    "        ax[i].set_ylabel(keys[i], fontsize=20)\n",
    "        ax[i].tick_params(labelsize=16)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "     \n",
    "    # Validation\n",
    "    y_true_valid      = np.expand_dims(np.asarray(valid_data_generator[0][1]), -1)\n",
    "    predictions_valid = np.asarray(model.predict(valid_data_generator))\n",
    "    eval_valid        = np.concatenate([predictions_valid, y_true_valid], 1)\n",
    "    \n",
    "    ################################\n",
    "    ######### Persistance ##########\n",
    "    ################################\n",
    "    \n",
    "    if persistance:\n",
    "        weights_file    = 'results/weights_model_{:s}'.format(str(est+1))\n",
    "        history_file    = 'results/history_model_{:s}.npy'.format(str(est+1))\n",
    "        valid_eval_file = 'results/valid_eval_model_{:s}.npy'.format(str(est+1))\n",
    "\n",
    "        print('\\nSave weights:               {:s}'.format(weights_file))\n",
    "        print('Save history:               {:s}'.format(history_file))\n",
    "        print('Save validation evaluation: {:s}'.format(valid_eval_file))\n",
    "\n",
    "        model.save_weights(weights_file)\n",
    "        np.save(history_file,    history)\n",
    "        np.save(valid_eval_file, eval_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
